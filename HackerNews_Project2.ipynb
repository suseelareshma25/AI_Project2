{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import nltk.stem\n",
    "import nltk.tokenize\n",
    "import copy\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_types_based_data(data, post_types):\n",
    "    post_type_vocab = {}\n",
    "    for post_type in post_types:\n",
    "        filtered_titles = data[data['Post Type'] == post_type]\n",
    "        all_words = ' '.join(filtered_titles['Title'])\n",
    "        all_words = all_words.lower()\n",
    "        tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "        vocabulary = tokenizer.tokenize(all_words)\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        lemmatized_vocab = [lemmatizer.lemmatize(w) for w in vocabulary]\n",
    "        post_type_vocab[post_type] = lemmatized_vocab\n",
    "    return post_type_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(file_2018, training_data, post_types, vocab,smoothing):\n",
    "    \n",
    "    fdist1 = {}\n",
    "    size_post_type_vocab = {}\n",
    "    model_dataframe = []\n",
    "    post_type_vocab = get_post_types_based_data(training_data, post_types)\n",
    "    for post_type in post_types:\n",
    "        size_post_type_vocab[post_type] = len(post_type_vocab[post_type])\n",
    "        fdist1[post_type] = nltk.FreqDist(post_type_vocab[post_type])\n",
    "    with open(file_2018, \"a\",encoding=\"utf-8\") as text_file:\n",
    "        for index,word in enumerate(vocab):\n",
    "            add_to_file = \"%d  %s\"%(index+1, word)\n",
    "            for post_type in post_types:\n",
    "                word_ptype = fdist1[post_type]\n",
    "                freq_word_ptype = word_ptype[word]\n",
    "                cond_probability = (freq_word_ptype + smoothing) / (size_post_type_vocab[post_type] + (len(vocab) * smoothing))\n",
    "                add_to_file = \"%s  %d  %.10f\"%(add_to_file, freq_word_ptype, cond_probability)\n",
    "            model_dataframe.append(add_to_file.split('  '))\n",
    "            text_file.write(add_to_file + \"\\n\")\n",
    "            \n",
    "    return pd.DataFrame(model_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_classifier(data_dataframe, DataFrameFile,test_data ,vocab, post_types,training_data):    \n",
    "    \n",
    "    test_data_post_type = {}\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    all_sentences = test_data['Title'].tolist()\n",
    "    test_data_vocab = ' '.join(test_data['Title'])\n",
    "    lemmatized_all_sentences = [[lemmatizer.lemmatize(word.lower()) for word in tokenizer.tokenize(s)] for s in all_sentences]\n",
    "    lemmatize_test_words = [lemmatizer.lemmatize(word.lower()) for word in test_data_vocab.split(' ')]\n",
    "    test_data_df = pd.DataFrame(lemmatized_all_sentences)\n",
    "    test_data_df = test_data_df.replace([None], 1)\n",
    "    \n",
    "#     print(test_data_df.head())\n",
    "#     print(data_dataframe.head())\n",
    "    for post_type in post_types:\n",
    "        test_data_post_type[post_type] = copy.deepcopy(test_data_df)\n",
    "    \n",
    "    \n",
    "    probabilities_post_types = probabilities_of_post_types(training_data)\n",
    "    \n",
    "    for word in lemmatize_test_words:\n",
    "        for index, post_type in enumerate(post_types):\n",
    "            row = data_dataframe[data_dataframe[1] == word]\n",
    "            if(len(row) != 0):\n",
    "                prob = row[(2 * (index+1) +1)]\n",
    "                test_data_post_type[post_type] = test_data_post_type[post_type].replace(word, prob)\n",
    "    \n",
    "    for post_type in post_types:\n",
    "        post_data = test_data_post_type[post_type]\n",
    "        post_data = post_data.apply(pd.to_numeric, errors='coerce')\n",
    "        test_data_post_type[post_type] = post_data.replace([np.nan], 1)\n",
    "        \n",
    "    print(\"Added the probabilities with post type\")\n",
    "    scores_data = np.zeros([len(test_data['Title']), len(post_types)])\n",
    "    for index, post_type in enumerate(post_types):\n",
    "        test_data_post_type[post_type] = probabilities_post_types[post_type] + np.sum(np.log10(test_data_post_type[post_type]), axis=1)\n",
    "        scores_data[:, index] = test_data_post_type[post_type]\n",
    "    \n",
    "    best_scores = np.argmax(scores_data, axis=1).astype('str')\n",
    "    for index, post_type in enumerate(post_types):\n",
    "        best_scores[best_scores == str(index)] = post_type\n",
    "#     print(best_scores)\n",
    "    #create_file(\"baseline-result.txt\", training_data, post_types, vocab)\n",
    "    \n",
    "#     for index, post_type in enumerate(post_types):\n",
    "#         for i in test_data_post_type[post_type].columns:\n",
    "#             merge_data = pd.merge(test_data_post_type[post_type], data_dataframe, how='left',left_on=[i], right_on=[data_dataframe.keys()[1]])\n",
    "#             test_data_post_type[post_type][i] = merge_data[merge_data.keys()[(-2)*(len(post_types) - post_types.index(post_type)) + 1]]\n",
    "    \n",
    "#     for post_type in post_types:\n",
    "#         #print(post_type)\n",
    "#         print('-----------------------')\n",
    "#         print(test_data_post_type[post_type])\n",
    "    data=np.column_stack((np.arange(1,len(test_data['Title'])+1), test_data['Title'], best_scores,scores_data,test_data['Post Type'], np.where(best_scores == test_data['Post Type'], \"right\", \"wrong\")))\n",
    "    np.savetxt(DataFrameFile, data, fmt='%s', encoding='utf-8', delimiter='  ')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilities_of_post_types(data):\n",
    "    fdist = {}\n",
    "    probability = {}\n",
    "    all_words = ' '.join(data['Post Type'])\n",
    "    all_words = all_words.lower()\n",
    "    word_list = nltk.word_tokenize(all_words)\n",
    "    total_post_types = len(word_list)\n",
    "    fdist = nltk.FreqDist(word_list)\n",
    "    for word,freq in fdist.items():\n",
    "        cond_prob = int(freq)/total_post_types\n",
    "        probability[word] = cond_prob\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading hn2018_2019 file\n",
      "Separating training and testing data\n",
      "Generating vocabulary\n",
      "Creating model-2018 file\n",
      "Added the probabilities with post type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91950\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: RuntimeWarning: divide by zero encountered in log10\n"
     ]
    }
   ],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "    # ******** READING FILE **********\n",
    "print(\"Reading hn2018_2019 file\")\n",
    "dataFile = pd.read_csv('hn2018_2019.csv')\n",
    "    \n",
    "dataFile.drop(\"Unnamed: 0\", axis=1, inplace=True,)\n",
    "    #print(dataFile)\n",
    "print(\"Separating training and testing data\")\n",
    "    # ******* TRAINING DATA **********\n",
    "training_data = dataFile[dataFile['Created At'].str.contains(\"2018\")]\n",
    "    \n",
    "    # ******* TESTING DATA ***********\n",
    "testing_data = dataFile[dataFile['Created At'].str.contains(\"2019\")]\n",
    "    \n",
    "    # ******* POST TYPES **********\n",
    "post_types=dataFile['Post Type'].unique()\n",
    "post_types = post_types.tolist()\n",
    "    #print(post_types)\n",
    "    \n",
    "    #probabilities_of_post_types(training_data)\n",
    "    \n",
    "    # ******* GENERATING VOCABULARY **********\n",
    "print(\"Generating vocabulary\")\n",
    "all_words = ' '.join(training_data['Title'])\n",
    "    #print(all_words[0:100])\n",
    "all_words = all_words.lower()\n",
    "    \n",
    "    #print(all_words)\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "vocabulary_eighteen = tokenizer.tokenize(all_words)\n",
    "    \n",
    "#     vocabulary_eighteen = nltk.word_tokenize(content)\n",
    "temp_set = set(vocabulary_eighteen)\n",
    "unique_vocabulary_eighteen = list(temp_set)\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "lemmatized_vocab = [lemmatizer.lemmatize(w) for w in unique_vocabulary_eighteen]\n",
    "lemmatized_vocab.sort()\n",
    "    #print(lemmatized_vocab)\n",
    "print(\"Creating model-2018 file\")\n",
    "vocab_dataframe = create_file(\"model-2018.txt\", training_data, post_types, lemmatized_vocab,0.5)\n",
    "naive_bayes_classifier(vocab_dataframe,\"baseline-result.txt\",testing_data[0:200], lemmatized_vocab,post_types, training_data)\n",
    "#     print(vocab_dataframe)\n",
    "#     vocab_smoothing_dataframe = create_file_GradSmoothingValue(\"model-2018-tt.txt\",training_data, post_types, lemmatized_vocab)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added the probabilities with post type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91950\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: RuntimeWarning: divide by zero encountered in log10\n"
     ]
    }
   ],
   "source": [
    "#Stopwords\n",
    "#def removeSoftWords():\n",
    "stop_words_file=pd.read_csv(\"Stopwords.txt\")\n",
    "stop_words_file=pd.DataFrame(\n",
    "np.row_stack([stop_words_file.columns, stop_words_file.values]),\n",
    "columns=['Stopwords'])\n",
    "stopwords_list=stop_words_file['Stopwords'].tolist()\n",
    "#print(stopwords_list)\n",
    "#stopwords_list=stop_words_file.split(\"\\n\")\n",
    "filtered_data=[w for w in lemmatized_vocab if w not in stopwords_list]\n",
    "#print(filtered_data)\n",
    "stopwordsFile = create_file('stopword-model-2018.txt', training_data, post_types, filtered_data)\n",
    "naivebayesstopwords = naive_bayes_classifier(stopwordsFile, \"stopword-result.txt\", testing_data[0:200], lemmatized_vocab,post_types, training_data)\n",
    "\n",
    "#print(stop_words_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added the probabilities with post type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91950\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: RuntimeWarning: divide by zero encountered in log10\n"
     ]
    }
   ],
   "source": [
    "#WordLengthFiltering\n",
    "\n",
    "filtered_data=[w for w in lemmatized_vocab if len(w)> 2 and len(w) <9]\n",
    "#print(filtered_data)\n",
    "stopwordsFile = create_file('wordlength-model.txt', training_data, post_types, filtered_data)\n",
    "naivebayesstopwords = naive_bayes_classifier(stopwordsFile, \"wordlength-result.txt\", testing_data[0:200], lemmatized_vocab,post_types, training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added the probabilities with post type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91950\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: RuntimeWarning: divide by zero encountered in log10\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-143-fad8d577498d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mwords_after_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords_freq\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mfreq_dist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mfrequencyModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"frequency-model\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpost_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords_after_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mfrequencyResult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnaive_bayes_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrequencyModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"frequency-result\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmatized_vocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpost_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-98-5b11bda6e070>\u001b[0m in \u001b[0;36mnaive_bayes_classifier\u001b[1;34m(data_dataframe, DataFrameFile, test_data, vocab, post_types, training_data)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlemmatize_test_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpost_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpost_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_dataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_dataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "frequency_list=[1,5,10,15,20]\n",
    "freq_dist = nltk.FreqDist(lemmatized_vocab)\n",
    "words_freq = copy.deepcopy(lemmatized_vocab)\n",
    "accuracy_list = []\n",
    "for freq in frequency_list:\n",
    "    words_after_freq = [word for word in words_freq if freq_dist[word] > freq]\n",
    "    frequencyModel = create_file(\"frequency-model\"+str(freq)+\".txt\", training_data, post_types, words_after_freq,0.5)\n",
    "    frequencyResult = naive_bayes_classifier(frequencyModel, \"frequency-result\"+str(freq)+\".txt\", testing_data[0:200], lemmatized_vocab,post_types, training_data)\n",
    "    \n",
    "    \n",
    "    #acc=accuracy_score(smoothingResultArrayTemp,smoothingResultArray)\n",
    "    #accuracy_list.append(acc)\n",
    "#plt.plot(frequency_list, accuracy_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3840.7000000000003\n",
      "Added the probabilities with post type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91950\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: RuntimeWarning: divide by zero encountered in log10\n"
     ]
    }
   ],
   "source": [
    "freqPercentage = [5, 10, 15, 20, 25]\n",
    "vocabWords = copy.deepcopy(lemmatized_vocab)\n",
    "freq_data = nltk.FreqDist(lemmatized_vocab)\n",
    "\n",
    "accuracy_list = []\n",
    "for freq in freqPercentage[0:1]:\n",
    "    freq_words = (freq / 100) * len(lemmatized_vocab)\n",
    "    print(freq_words)\n",
    "    most_freq_words = freq_data.most_common(int(freq_words))\n",
    "    filtered_words = [word[0] for word in most_freq_words]\n",
    "    words = [word for word in vocabWords if word not in filtered_words]\n",
    "    freqModel = create_file('freq-model.txt', training_data, post_types, words,0.5)\n",
    "    freqResult = naive_bayes_classifier(freqModel, \"freq-result.txt\", testing_data[0:200], lemmatized_vocab,post_types, training_data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added the probabilities with post type\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91950\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: RuntimeWarning: divide by zero encountered in log10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added the probabilities with post type\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "Added the probabilities with post type\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "Added the probabilities with post type\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "Added the probabilities with post type\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "Added the probabilities with post type\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "Added the probabilities with post type\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-426a6ddf5d4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msmoothing_values\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0msmoothingModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'smoothing-model.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpost_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0msmoothingResult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnaive_bayes_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmoothingModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"smoothing-result.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmatized_vocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpost_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;31m#print(\"--------------\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m#print(\"------------\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-98-5b11bda6e070>\u001b[0m in \u001b[0;36mnaive_bayes_classifier\u001b[1;34m(data_dataframe, DataFrameFile, test_data, vocab, post_types, training_data)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlemmatize_test_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpost_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpost_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_dataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_dataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, other, axis)\u001b[0m\n\u001b[0;32m   1764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1765\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1766\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1767\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1768\u001b[0m                 raise TypeError('Could not compare {typ} type with Series'\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mna_op\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1624\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1625\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_comp_method_OBJECT_ARRAY\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1627\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_datetimelike_v_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Smoothing\n",
    "smoothing_values=[]\n",
    "accuracy_score=[]\n",
    "for i in np.arange(0, 1,0.1):\n",
    "    rounded_smoothing=float(round(i,2))\n",
    "    smoothing_values.append(rounded_smoothing)\n",
    "#print(smoothing_values)\n",
    "for i in smoothing_values:\n",
    "    smoothingModel = create_file('smoothing-model.txt', training_data, post_types, filtered_data,i)\n",
    "    smoothingResult = naive_bayes_classifier(smoothingModel, \"smoothing-result.txt\", testing_data[0:200], lemmatized_vocab,post_types, training_data)\n",
    "    #print(\"--------------\")\n",
    "    #print(\"------------\")\n",
    "    print(type(smoothingResult), type(smoothingResult[:,7]), type(smoothingResult[:,2]))\n",
    "    acc=sklearn.metrics.accuracy_score(smoothingResult[:,7],smoothingResult[:,2])\n",
    "    #print(acc)\n",
    "    accuracy_score.append(acc)\n",
    "plt.plot(accuracy_score,smoothing_values)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
